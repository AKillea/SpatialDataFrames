{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas and SQL\n",
    "Structured Query Language, or **SQL**, is the language of databases. Though \"SQL\" may seem new to you, you've actually used it in your GIS work, specifically in selecting records by attribute values. SQL, however, is capable of much more than just selecting records; SQL is used to create, read, relate (i.e. join), update, delete records stored in database tables. \n",
    "\n",
    "In this exercise we use Pandas to mimic many SQL functions, showing that we don't need to learn daunting database management systems like Oracle, Microsoft SQL Server, PostgreSQL, or even MS Access, to run some of the everyday database functions you might want to run. Instead, we can import data as a dataframe and use some Pandas functions to get what you need. \n",
    "\n",
    "Here we explore some useful data query and transformation techniques are applied to marine species observations data extracted from OBIS/SeaMap. These data are stored in the data folder as `Seamap400.csv`. \n",
    "\n",
    "→ *You may want to examine this file in Excel to familiarize yourself with the data first.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the modules\n",
    "import pandas as pd\n",
    "import numpy as np #Well need NumPy for some of its data types..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the csv data into a Pandas dataframe\n",
    "We've loaded CSV data into a Pandas dataframe in previous exercises. The code block below demonstrates how the `read_csv` function easily does this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the data into a dataframe called \"dfSeamap\"\n",
    "dfSeamap = pd.read_csv('../Data/Seamap400.csv')\n",
    "#Show the first 2 records in the result\n",
    "dfSeamap.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result *looks* good, and indeed we can do much with this dataframe. BUT, there are some \"gotchas\" that we can fix:\n",
    "* First, Pandas assigns its own index, when we might want to keep the ID field as the index values.\n",
    "* The `dataset` and `rowid` fields, are actually nominal values, not actual numbers, and we should import these as strings, not numbers (in case they have a leading zero, for example).\n",
    "* The `last_mod` and `obs_date` fields are imported as string objects, but we can import these as _date_ object, which will allow for selection via _time slices_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So... to fix this, we can specify a few extra parameters when importing the data. Below is the `read_csv()` function applied as above, but with some additional parameters (and also reformatted to make this statement more readable):\n",
    "* The `index_col` specifies which column will be our dataFrame's **index**. \n",
    "* `parse_dates` indicates that the `last_mod` and `obs_date` columns are a **date fields**, not strings, which enables us to to time-based queries. \n",
    "* And finally, the `dtype` functions allows us to override the defaut data type assignments. Here, we pass a dictionary consisting of column names (keys) and data types (values) to indicate which columns:data type pairs we want to specifiy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the Seamap400.csv file to a DataFrame\n",
    "dfSeamap = pd.read_csv(\n",
    "    '../Data/Seamap400.csv',             # File containing the data\n",
    "    index_col = 'ID',                    # Which column to set to the dataframe's index\n",
    "    parse_dates=['last_mod','obs_date'], # Which columns to format as dates\n",
    "    dtype={'datasetid':'str',            # Coerce the 'datasetid' and 'rowid' columns to be strings\n",
    "           'rowid':'str'\n",
    "          }\n",
    ")\n",
    "#Display the first two rows\n",
    "dfSeamap.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than the ID now being used as the index the result looks the same, but if you inspect the data types, you'll see we got what we wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the data types of the import\n",
    "dfSeamap.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring our dataFrame\n",
    "Now, let's examine how many unique species there are. This is analogous to the `SELECT DISTINCT` SQL operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSeamap['sp_common'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting <u>columns</u> of data from a dataFrame\n",
    "Selecting columns is analogous to the SQL `SELECT ... FROM` clause, but the syntax in Pandas is a bit different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ♦ Selecting data from <u>a single column</u> in the dataframe\n",
    "Selecting a single column is easy as specifying the column name, either between brackets with the field name in quotes, or using the dot notation. (*The former is preferred, in case your field name is the the same as another property or method of a dataFrame.*)\n",
    "\n",
    "When only one column is selected, the object returned is actually not a dataFrame, but a pandas **series** object, which is quite similar to a one-dimensional NumPy array. (However, in Pandas, series are limited to one dimension...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select just the sp_common field into a new view\n",
    "dfSelect = dfSeamap['sp_common']\n",
    "dfSelect.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using \"dot notation\" (<dataframe>.<field name>) also works\n",
    "commonNames = dfSeamap.sp_common\n",
    "type(commonNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As the object returned is a series, we can get records from its index\n",
    "commonNames[:8] #Returns the first 8 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What code would you use to get the last 8 records? (Replace the █)\n",
    "commonNames[█]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ♦ Selecting data  from <u>multiple</u> columns\n",
    "Selecting multiple columns can be done by including a list of the column names. *Note the double set of brackets: the outer set is the syntax of the dataFrame object, and the inner set denotes the list of field names we want.*\n",
    "\n",
    "Here, the object returned resembles a dataFrame, not a series (since Pandas' series can only be one-dimension). However, in actuality, this is simply a **view** of the original dataFrame, not a new object - much like query views in SQL..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the 'sp_common' and 'sp_class' columns into a new view\n",
    "dfSelect = dfSeamap[['sp_common','sp_class']]\n",
    "dfSelect.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##What code would create a table of just the species common name and the latitude and longitude columns?\n",
    "dfSelect2 = dfSeamap[[█]]\n",
    "dfSelect2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting <u>rows</u> in a dataFrame\n",
    "Selecting rows, analogous to the `select...where` statement in SQL, can be done by creating **boolean masks** of the criteria you want and then applying those masks. This can be done explicitily as two steps, or more commonly in a single, compount step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before making our selection, let's examine how we can easily extract a list of valid options using the `unique` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List the unique values in the 'sp_class' field.\n",
    "dfSeamap['sp_class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display just the *number* of unique latitude values\n",
    "dfSeamap['latitude'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting rows in two steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Create the boolean mask\n",
    "theMask = dfSeamap['sp_class'] == 'Mammalia'\n",
    "theMask[2150:2156] #Show a selection of records, to display values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Applying the mask to return only 'true' records\n",
    "dfMammals = dfSeamap[theMask]\n",
    "dfMammals.sample(4) #Show a random sample from the product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Proof that we only got mammals in the result\n",
    "dfMammals['sp_class'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting rows in one step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all the mammals and report all unique scientific names\n",
    "dfMammals = dfSeamap[dfSeamap['sp_class'] == 'Mammalia']\n",
    "dfMammals.shape #Reveal the size of the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMammals['scientific'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting rows using `query`:\n",
    "We can also execute familiar **query strings** to extract records. *Personally, I don't like this method as the query strings tend to be finicky and can be tough to use with quotes..*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMammals = dfSeamap.query(\"sp_class == 'Mammalia'\")\n",
    "dfMammals.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complex row selections\n",
    "We can combine masks using logical operators (`&` = \"and\", `|` = \"or\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select mammal records in the 2nd half of 2006\n",
    "mammalMask = dfSeamap['sp_class'] == 'Mammalia'\n",
    "\n",
    "#Create date objects for the start and end dates\n",
    "startDate = np.datetime64('2006-07-01')\n",
    "endDate = np.datetime64('2007-01-01')\n",
    "\n",
    "#Create the date masks\n",
    "startMask = (dfSeamap['obs_date'] >= startDate) \n",
    "endMask = (dfSeamap['obs_date'] < endDate)\n",
    "\n",
    "#Apply the masks, using the bitwise '&' to return rows where all masks are true\n",
    "dfSelect2 = dfSeamap[mammalMask & startMask & endMask]\n",
    "dfSelect2['scientific'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Return rows matching a substring\n",
    "The `.str` function on a column allows us to use some string operations  on the values in that field.  Here we use the string `startswith` function to return all rows where the row's value starts with 'Delphin'. See https://pandas.pydata.org/pandas-docs/stable/text.html for other string operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select rows where the scientific name starts with \"Delphin\" \n",
    "dolphinMask = dfSeamap['scientific'].str.startswith('Delphin')\n",
    "dfDolphins = dfSeamap[dolphinMask]\n",
    "\n",
    "#Use the nunique function to just return the number \n",
    "#  of unique scientific names\n",
    "dfDolphins['scientific'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More complex queries with `apply` and  `lambda`\n",
    "For maximum flexibility, we can actually write our own functions to be applied to each value in a column (or multiple columns). This is done using the `apply` function to a dataFrame and then specifying the subcode we want to use with Python's `lambda` statement. (This seems fairly complex at first, but it actually somewhat straightforward -- and can be very useful...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a mask by searching each row for the string \"Whale\" and apply the mask\n",
    "# to list the scientific names of these records\n",
    "whaleMask = dfSeamap['sp_common'].apply(lambda x: 'Whale' in x)\n",
    "dfWhale = dfSeamap[whaleMask]\n",
    "dfWhale['scientific'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping/Aggregating data\n",
    "Pandas can aggregate data on values like SQL as well. We do this with the `groupby` statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the observations by common name\n",
    "grpSpCommon = dfSeamap.groupby('sp_common')\n",
    "grpSpCommon['sp_common'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also show *all* the summary stats with the dataFrame's `describe` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grpSpCommon.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping tables\n",
    "In addition to grouping data, we can \"pivot\" the data, summarizing by one field and aggregating values across other fields. Let's look at an example where we display "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a copy of the data\n",
    "dfSeamapCopy = dfSeamap.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a \"day\" column (https://pandas.pydata.org/pandas-docs/stable/api.html#datetimelike-properties)\n",
    "dfSeamapCopy['week_num'] = dfSeamapCopy['obs_date'].dt.weekofyear\n",
    "dfSeamapCopy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pivot to a table listing row=species, col = day of year, values = latitude\n",
    "dfLat = dfSeamapCopy.pivot_table(index='sp_common',columns='week_num',values='latitude')\n",
    "dfLat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the longitudinal travel across weeks of a given species\n",
    "dfLat.loc['Bottlenose Dolphin'].plot(kind='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining tables\n",
    "Also like SQL, pandas can join tables. Below we'll create two tables from our aggregated data: one will list the minimum of the latitude and longitude columns, and the second will list the maximum values. Then we'll join these two tables and compute the geographic extent of each species observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataFrames of the minimum and then maximum of the lat and lng fields\n",
    "minCoords = grpSpCommon['latitude','longitude'].min()\n",
    "maxCoords = grpSpCommon['latitude','longitude'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at what is returned (for the min table)\n",
    "minCoords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the Pandas 'merge' command to join the two tables\n",
    "sppExtent = pd.merge(left=minCoords,     #Specifies the left table\n",
    "                     right=maxCoords,     #Specifies the right table\n",
    "                     how = 'inner',      #Specifies the type of join\n",
    "                     left_index=True,    #Use the index of the left table as the join item\n",
    "                     right_index=True)   #Use the index of the right table as the join item\n",
    "#Have a look\n",
    "sppExtent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename the columns to the values in the list provided\n",
    "sppExtent.columns = ['minX','minY','maxX','maxY']\n",
    "sppExtent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute two new columns as the difference between max and min\n",
    "sppExtent['XRange'] = sppExtent.maxX - sppExtent.minX\n",
    "sppExtent['YRange'] = sppExtent.maxY - sppExtent.minY\n",
    "sppExtent.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
